# Methods

In writing up a project informed by traditions of narrative and situated learning, it would be hypocritical to start my writeup as if participant selection, method, and so forth sprang into formal, fully-formed writeups before the process began. Instead, I have written the methods section in a fashion that is in keeping with, and representative of, the research methodology that I used. This chapter is a momentary freeze-frame of the words I’ve chosen for this project’s story now. Following the postmodern tradition, I will be inviting you to interrogate this text in a readerly manner by occasionally blurring the lines between “description of procedure” and “narrative of Mel.” My intent in writing the methods section this way is to show some of the complexity and mess behind my process instead of attempting to clean it all up in hindsight.

The first recognizable proposal draft started -- as I remember it -- somewhere between coffee and Indian food with Matt Jadud, the 4th member of my graduate committee. It was November 2012, and Matt was in town for  my  Readiness Assesment defense, my department’s rough equivalent of qualifying exams. We had met when he was a visting professor at my alma mater, Olin College. At some point, mingled between cappucinos and palak paneer, Matt remarked on the similarities between Olin and his current division, Technology and Applied Design (TAD) at Berea College. Both programs had invented or reinvented their undergraduate curriculum to focus on “design” as a continuous 4-year theme. It would be an intriguing parallel to investigate, suggested Matt.

At that moment, I wasn’t thinking about “points of commonality for thematic coding” or finding prior literature to “tie the cases examined into the larger context,” although these are phrases I’ve used in describing the study in hindsight. My state of mind was more focused on:
Wow, this tamarind chutney is tasty.
Ooh, that would be an excellent excuse to hang out with interesting people.

The “interesting people,” in this case, were TAD and Olin faculty, whom I knew through personal experience to be thoughtful teachers and compelling storytellers. If I constrained myself to their narratives as a starting point, what sort of project could I make?

This chapter describes the methods of my study, including participants, procedures, and positionality for the project as a narrative told in hindsight. It’s tempting to pretend the vision was clear all along, when it was not. I made choices about the project in the midst of what often felt like turbulence, and in response to things I knew I did not fully understand. Just like several of the faculty narratives you will read in the following chapters, there was a constant rummaging for words and trying out theories to see if they fit what I was already in the middle of doing.

The first part of this chapter describes the participants, since my specific set of narrators was the first thing to solidify in the study. The second part locates this study within the tradition of narrative research and details my procedures for data collection; the third part does likewise for analysis.

## Participants

In this section, I introduce the 6 faculty narrators and how they came to participate in the project. 

### Deciding on narrative interview methodology

Once Matt broached TAD and Olin’s design curricula as a possible parallel, I realized I’d need to find a way to look at “faculty as learners” within that space. Since much of the curricular revision for both schools had occurred over stretches of time in the past, asking faculty to tell their stories of that past seemed like a fruitful way to look at changes in their viewpoints over time. As characters in their own stories, they would undergo developmental arcs -- in other words, even if they focused on their curricular design and teaching, they would be telling stories of their own informal learning. This choice situated the project in the narrative interview methodology.

### Tensions in bounding the interview design

The next two design decisions were heavily entangled: what to center the interviews around and how many narrators to select. The bounds of the central theme needed to be specific enough to give the narrators substantial commonality, and yet not so restrictive that I could not find participants. The number of participants needed to be large enough to gain multiple perspectives on the topic, yet small enough to make in-depth inquiry manageable.

### The common theme: design thinking curriculum revision at a small undergraduate teaching college

Working across two institutions was a way to see if narratives could inform our understanding of faculty-as-learners in a way that would transfer across institutions. In order to gather narratives with a point of commonality for thematic coding, I needed to articulate the parallels between TAD and Olin that I wished to investigate. The phrase “design thinking curriculum revision” describes a process common to both schools; design thinking was the pedagogical focus both TAD and Olin had deemed important enough to overhaul the rest of their curriculum around.

The choice of design thinking curriculum revisions as a narrative focus is semi-arbitrary; it could have been another shared experience such as “mentoring senior capstone projects” or “integrating writing across the curriculum” while still addressing the research question of faculty-as-learners. In other words, the point of commonality is an artifact rather than the focus of analysis; this study focuses on understanding faculty-as-learners rather than how faculty-as-learners make sense of design thinking curriculum revisions.
More about the institutions

In addition to design thinking curriculum revisions as a common process experience, TAD and Olin share several qualities relevant to this study. TAD is a division within Berea College; both Berea and Olin are small (<2000) suburban undergraduate colleges with a strong teaching focus and an extraordinary scholarship policy that attracts an unusual student body at each. These and other factors mean that TAD and Olin faculty must think especially critically about their curriculum designs, since their student body is “nontypical” compared to most engineering/technology undergraduate populations.

Berea is a small liberal arts teaching college in Berea, Kentucky with approximately 1,600 undergraduate students. Founded in 1855 as the first interracial and coeducational college in the South, Berea awards a full-tuition scholarship to all students, who are required to have a certain level of economic need in order to be admitted. At the time of data collection, the Technology and Applied Design (TAD) program had 5 faculty members (2 of whom were on 50% appointments split with other departments) and offered concentrations in Technology Management, Artisan Studies, and Engineering and Technology Education. TAD had recently completed a self-study that led to a departmental renaming and curricular redesign emphasizing “design thinking” across the entire 4-year curriculum.

Olin is a small engineering college in Needham, Massachusetts with approximately 350 undergraduates Olin provided full-tuition scholarships to all students prior to the class of 2014, and has a nearly even balance of male and female students, which is rare in engineering. There are fewer than 40 total faculty members. Since Olin was founded in 2007 and many of the founding faculty were young, many founding faculty are still at Olin and approaching mid-career today. Olin needed to build its engineering curriculum from scratch before the first students in 2001, a year prior to the start of classes. These students had a substantial impact on the development of the “design stream” of their engineering program, which emphasizes user-centered design.

Additionally, and equally importantly, I had existing relationships with faculty at both institutions. This made it easier to gain access to the level of trust and commitment needed for an intensive narrative study. Olin is my alma mater; I earned my B.S. in Electrical and Computer Engineering there in 2007, and had kept in touch with faculty there ever since. I had taken their classes, eaten at their houses, and gotten to know their kids. My connection to TAD faculty was not as deep-rooted, since there’s no substitute for watching someone grow from an eager teenager into a young adult. However, I had gotten to know them through working with Matt on redesigning their electronics course to fit the “design thinking” curriculum revision thrust. I had also gone with them to an on-campus workshop on curricular design, eaten dinner with them at division social outings, and so forth, and they knew me as a graduate student who was working with their colleague Matt. 

### Three narrators form an intertextual trio at each institution

The focus on design thinking curriculum revisions meant I would need narrators to focus on their participation in their school’s process, which had been a collaborative effort at each institution. Informed by the perspectives of situated and communal learning, I realized that a single-narrator case study from each institution could only talk about the intersubjective interactions wherein they and their colleagues shifted their viewpoints in response to one another. If I wanted the study to directly show intersubjectivity, I needed multiple narrators so they could respond to each other’s stories and insights directly in their interview transcripts. Having 3 narrators from each institution was the minimum number that would allow for a within-institution version of the intertextual technique of asking one narrator to compare and contrast narratives from two others. 

### Criteria for narrator participation

In order to ensure all faculty narrators could contribute in a similar manner to the narrative accrual on design thinking curriculum revision, I came up with the following requirements for participants:

Must have been deeply involved in the curricular design process at their respective institution, so they could speak about their direct experiences and decisions in that process instead of theorizing from secondhand knowledge.
Must have subsequently taught courses affected by the revision, so they could speak to the learning that occurs when one’s design collides with reality for the first time.
Must still be actively collaborating on curricular-related matters with other study participants from the same institution. This way, the collaborative sense-making process could continue outside the bounds of our formal narrative interviews. Although the study was not designed as a formal intervention, I did not know whether or how participating in the interviews would affect how faculty narrators went about their everyday work. In case it did, I wanted narrators to be able to speak with each other about that process as easily as possible without my intervention. In practice, this meant TAD narrators were still at TAD and Olin narrators were still at Olin; nobody had moved on to a different institution at the time of the interviews.
Must be willing to be publicly identified in research artifacts.

### Initial discussions on research transparency

The last criteria was discussed informally at length with potential narrators while the study design was being finalized. Because of the small size and unique nature of their institutions, I realized early on that faculty narrators would be highly identifiable. The conventional practice of anonymizing both participants and institutions would destroy the narrative particularity emphasized by Bruner (1991, p. 6-7).

Part of the study design draft I discussed with potential narrators was the idea of radically transparent research, which I will discuss in a later section of this chapter. I worried that narrators would be uncomfortable with removing anonymity, but all of them said they were willing to be part of a public research process where their names would be released alongside their data. I needed to know if all 6 narrators would be comfortable with that part of the study design, because it would determine how I collected, shared, and worked with data -- if even one narrator was not comfortable with transparency, I would have designed the study differently in order to protect confidentiality. Since they were all willing, the transparency criteria stayed in place and I designed the protocol accounting for it.

### Bonus commonality: education backgrounds

Although the three points listed above were the only eligibility criteria for narrators, there ended up being other inadvertent areas of commonality. All participating faculty happen to be cross-disciplinary. Each of them has both a formal academic background in engineering or technology and either a formal credential or current research focus in engineering or technology education. The cross-disciplinary and education-related backgrounds of participants was an unforeseen advantage that allowed our conversations to delve deeper into shared meaning-making than may otherwise have been possible, because narrators had theoretical vocabularies for learning that they then used to describe the learning f both their students and themselves.

### How participants were recruited: TAD

The nature of the TAD faculty group was very much a consideration in negotiating the boundaries for thematic focus and number of narrators. For example, I could not have had more than 5 narrators per school, for the simple reason that TAD had 5 faculty members at the time of the study’s design. Three had been around TAD for some time, while two were much newer. These two groups represented distinct clusters of levels of involvement in the curricular redesign.

During the process of iterating on these study design boundaries, I used my existing relationship with the 3 more experienced TAD faculty and reached out informally for feedback on the current rough version of the study design, and to see if they might be willing to participate as narrators. All three gave an informal yes to my informal ask, so I continued tinkering with the study design under the loose assumption that they would be my TAD narrators. As a consequence of these conversations, I bounded the criteria in such a way that it fit the three more experienced faculty members.

### How participants were recruited: Olin

3 narrators from TAD meant I needed to find 3 narrators from Olin to provide a parallel. Taking the commonalities of the TAD faculty -- heavily involved in the design, taught curriculum affected by it, and still at the institution -- I translated them into the Olin context, which I knew from firsthand experience as an alumna. If the design thinking curriculum revision was the common thread, Olin narrators would have to have been at Olin since before the current design curriculum was started, which meant prior to the freshman year of the class of 2006. Using the faculty directory on Olin’s website, I gathered the emails of all the current Olin faculty who had been there at that time, and emailed those 11 people a similar informal and personal ask. Three responded with an immediate yes; they became the Olin narrators.

Having gotten a preliminary gauge of interest around the study and an informal indication that the “transparent” version of my study design might work, I wrote up the protocol and the remainder of the proposal and submitted it to Purdue’s IRB. After being approved, I sent out the “official” recruitment emails and got formal consent on paper from all 6 narrators. In many ways, this felt like a formality after the months of intermittent discussion with the narrators and their colleagues about the study design. 

### Introducing the narrators

Faculty narrators for this study had the opportunity to introduce themselves as characters at the beginning of their first interview. Here is a condensed version of what they said; note that different narrators emphasized different aspects of identity in their introduction, and varied widely on formality and level of detail.

“Mark” - Mark P. Mahoney, Assistant Professor of Technology and Applied Design at Berea College: I am an Assistant Professor at Berea College. I have primarily have been teaching courses that deal with materials, energy power, drafting and electricity over my 5 years of being here. I was a high school, middle school teacher for years. I was very happy at that position. I saw where the current technology education curriculum was going and wasn’t happy with it. I wanted to move to a level where I had an impact on it. I went into Ohio State and I studied technology education and STEM education as far as presenting the curriculum and multiple disciplines to improve the students education. Berea invited me down... the mission of the college was exceptional.... it is a rarity to find that kind of combination. So I have been here.

“Gary” - Gary S. Mahoney, Professor (and former Chair and alumnus, no relation to Mark) of Technology and Applied Design at Berea College: I’m a graduate [of Berea’s TAD program] and came back for teaching in 1989. I have a doctorate in education. I teach the design and production [in wood].  I teach [computer] aided design classes.  I teach the computer integrated manufacturing.

“Alan” - Alan D. Mills, Professor (and Chair) of Technology and Applied Design at Berea College: I'm a little older than Gary, about nine years. I'm heavily entrenched.  I came out of the really strong period of industrial arts in the '70s, when it was really, really thriving. It's been a big adjustment to pick up on the new technology and design concepts that we didn't have when I was in college. I can remember building a bookcase and we found a great drawing in a book that I liked a lot, but it was a little oversized for me. So I shrank all the measurements down and did some tweaking of it. In some sense, that's designing. But that would usually be the extent of it. We pretty much got our designs from somebody who had already designed something that we're trying to build.

“Jon” - Jonathan Stolk, Professor of Mechanical Engineering and Materials Science at Olin College: It is important for me to communicate to people that I arrived before students arrived at Olin... I always have to get that into my introduction... one of the things I oftentimes say is I design nontraditional learning experiences for a group of undergraduate engineering students.

“Rob” - Robert Martello, Professor of the History of Science and Technology at Olin College: I am currently a professor of the history of science and technology. I have a bachelor’s degree in a science discipline and a master’s degree in engineering before I entered my Ph.D. program in history... [so] by training I have sampled science, engineering... [and] the social sciences. I’m sort of weird... in my intellectual vitality space... in the sense that I have... [a] history side and [an] education side.

“Lynn” - Lynn Andrea Stein, Professor of Computer and Cognitive Science (and Associate Dean and Director of the Collaboratory) at Olin College: My background is in computer science and in cognitive science. I speak of myself as broady interdisciplinary person... I spend a lot of my life thinking about things at disciplinary boundaries. I was... one of the people with design background, particularly from my HCI (human-computer interaction) experience and cognitive science experience. I was a professor at MIT for 10 years before I left to help create a brand-new college.  I’ve been at Olin since before there were students... I was here a year before Rob and Jon.

In keeping with the blurred distinction of researcher and subject, here is my own introduction:

“Mel” - Mallory Chua, PhD candidate in Engineering Education at Purdue University: I’m an Olin alumna (class of 2007) who has assisted with curricular design at TAD, so I know the narrators above outside the context of this study. I tend to naturally act from a postmodernist perspective, which fits both my background in open source software and my lived experience as a deaf person constantly bricolaging and making-sense-of partial access to information. However, I didn’t realize that or know there was a word for it until one of my qualitative methods professors explained this as the reason I didn’t fit clearly into any other research paradigm (post-positivist, constructivist, critical theorist, etc.).

## Data collection process

### Overview of narrative analysis as a method

In the literature review earlier in this document, we explored the underlying philosophical perspectives of narrative analysis as a methodology. Here, we examine narrative analysis as a concrete method for carrying out research that is shaped by the underlying philosophical perspectives of the methodology.

Narrative analysis applies techniques from literary studies to the stories people tell in order to understand how people construct meaning from the world. Bruner (1991) describes narratives as things through which people construct social systems by expressing and extracting meaning, but which cannot be interpreted as sequences of strict causality because of the free will their characters possess. Instead, readers interpret the reasoning behind a story according to the context surrounding them, a context they may or may not be aware of. The communal interpretation of a narrative or accrual of narratives is thus a way to share and negotiate our differences in perspective without necessarily collapsing those differences. This is an important feature for this research project, which seeks to open and deepen dialogue across different institutional contexts on a topic whose interpretation is often tacit and personal and therefore difficult to transfer.

When our shared narratives are about our own actions, we end up engaging in the reflection-in-action (Schön, 1983) that cognitive apprenticeship literature so highly values (Collins, Brown, & Holum, 1991). Reflection-in-action is a place of generative tension; while standard and shared meanings are indeed reified by community narrative collections, Mishler (1986) also describes this participation in community discourse as helping us avoid the trap of assuming those standard meanings out of habit, and instead allowing meaning to emerge through and be realized in the discourse itself (p. 65). The aim is not to search “for the best or most authentic answer,” but rather to “systematically activate applicable ways of knowing – the possible answers... as diverse and contradictory as they might be” (Holstein & Gubrium, 1995, p. 37). “Our individual autobiographies... depend on being placed within a continuity provided by a constructed and shared social history in which we locate our Selves and individual continuities. It is a sense of belonging to this canonical past that permits us to form our own narratives of deviation while maintaining complicity with the canon" (Bruner, 1991, p. 20). This research project seeks to elicit such reflexive autobiographies on teaching and to help their narrators collaboratively place them into a shared social history that will be accessible to the public.

### Interview format and prompt design

Narrative elicitation lends itself to an open-ended interview format. In order to see whether and how their understanding of their narratives and characters unfolded over time, I needed to conduct multiple interviews with each narrator. Additionally, I wanted faculty to have multiple opportunities to iterate on their stories in response to others. This would be in keeping with Schoen’s emphasis on “reflection-in-action” (YYYY) and the importance of reflective critique (Glassick et al, YYYY).

Choosing a process of reflective critique across narrators would allow me to watch faculty learn about themselves and other faculty as learners. In other words, I didn’t just want to have faculty talk about themselves as learners in the past; I wanted to watch their process of making-sense of themselves and their colleagues as learners in the present, during the narrative process itself. This meant that a major study artifact would be a shifting, intersubjective narrative accrual in the process of being constantly built and rebuilt.

Consequently, I decided to have nearly all the interview prompts consist entirely of data from prior interviews. In this way, faculty narrators would spend their time reflecting on narratives and telling or re-telling their own narratives in response.

### Creating interview prompts

The first interview with each narrator served as a “seed” interview to gather material for future prompts. The prompt for the first interview consisted of two questions:

How would you like to introduce yourself as a character in the stories you’re about to tell us about curriculum design/revision?
Tell me the story of what happened during the curriculum design/revision you were a part of. (Probe for addtional detail/stories as needed for the remainder of the interview.)

All subsequent prompts were remixed excerpts from prior interviews. I curated these prompts, which varied between 1-7 pages in length, from the data pool that existed at the time of each interview. 

Although faculty members write intertextually all the time (when quoting references in scholarly papers) and have frequent intersubjective interactions (co-analyzing data on a research project), one concern was that it might be disorienting for them to become conscious of these behaviors and try to deliberately employ them in a narrative elicitation session. Therefore, I scaffolded this process by choosing which subsets of the existing narrative accrual to draw from when creating prompts. Prompt progression was as follows:

Solo interview
Reflection on solo interview (trasncript #1 from self)
Reflection on same-institution colleague interviews (transcripts #1-3 from narrators at the same institution)
Reflection on cross-institution interviews (transcripts #1-6 from narrators at the other institution)
Reflection on any interviews (transcripts #1-6 from narrators at any institution)
Same as #5, plus a preview of preliminary results

As a narrator progressed through the interview process, I drew their prompts from an increasingly wider pool of transcripts. The prompts were designed to move them from (1) responding to themselves to (2) responding to familiar colleagues from their institution to (3) responding to unfamiliar colleagues from a different institution. This allowed narrators to start by reflecting on topics closer to their own experience. 

Each narrator participated in at least 3 interviews and up to 6, depending on schedule considerations. This guaranteed each narrator’s stories would be actively intertextual since they would, at minimum, reach the point of reflecting on the stories of colleagues at their institution.

### Prompts as discourses

I created prompts by taking an inductive approach to the data, rummaging my grubby hands through the mess and playing with the connections that popped out. Prompts were not grouped by themes, which is the typical inductive approach to qualitative analysis. “Emergent themes” are a common “building block” for inductive analysis (Williams, 2008) and are often depicted as being passively allowed to emerge before being categorized. Instead, I actively linked the transcript texts into discourses. (Parker, 2004, p. 100-101). 

In contrast to the passive emergence of themes, poststructural texts are full of “pastiche, montage, collage, bricolage, and the deliberate conglomerizing of purposes,” all highly active events that “fight the tendency for our categories to congeal” (Lather, 1991, p.10, 13). Instead of sorting emergent ideas into as few buckets as possible, poststructuralists proliferate categories until there too many to fit neatly into an overarching metanarrative (Lyotard, 1984). Intertextuality and the self-referential nature of sign systems such as language mean that the act of linking into discourses does not have a set beginning or end; even a text with zero citations in APA format draws from ideas and vocabularies that existed before it.

Linking discourses endlessly weaves and re-weaves our data into and within the massive fabric of the world’s information, which leaves us with an overwhelming Library of Data We Haven’t Analyzed Yet, to adapt a phrase from Italo Calvino’s If On A Winter’s Night A Traveler (1982). Just as we enacted agential cuts around our data so that a set of data would exist for us to analyze, we must enact agential cuts within our data to snip out finite artifacts that are small enough to analyze. 

Discourses can look like themes or contain recurring thematic material; they can also look like narratives, dialogues, or any combination of the above. Discourses can display both shared and divergent perspectives. Discourses can merge and splinter from each other; they are not discrete and separate entities. In creating and curating these discourses, I attempted to filter for topics that seemed related to examining faculty as learners. 

### Discourses created for and with/in interview prompts

The following is a list of discourses I traced through the creation of interview prompts. These discourses show up in various forms in the results section.

The “introductions” discourse -- faculty deliberately describing themselves as characters, shifting their commenting on and comparing other faculty as characters in relation to each other and themselves.
Mark’s “thick skin” aha moment (which splintered off his characterization of self in the “introductions” discourse)
Lynn’s “sarcasm” aha moment (which splintered off the “Saga of UOCD” and intersects with the “introductions” discourse)
“The Saga of UOCD,” Lynn’s dramatic retelling of a course origin story, including the reactions of her colleagues to that story
“The Saga of 130,” Mark’s dramatic retelling of a course origin story, including the reactions of his colleagues and the Olin narrators to that story
The “Maker disagreement,” where Mark and Alan both explain their side of a conversation that happened at a TAD curricular meeting
“Getting on the same page” as a Sisyphean task involving groups (that is often geared towards “Aligning faculty and curriculum to help students succeed”)
“Aligning faculty and curriculum to help students succeed”
“Making sense in chaos,” which weaves into all the course origin stories
Jon’s “headache” from trying to reconcile the conflicting explanations he got about the Olin curriculum on his first day (intersects with “Getting on the same page”)
TAD’s “revolving door” turnover rate, as described by both Gary and Mark (intersects with “Getting on the same page”)
“We’ve always been” group identity history of TAD; they’ve always been applied and they’ve always taught design, teaching actively before it was in vogue
“TAD’s Name” (intersects with “We’ve always been” and “Other’s perceptions of TAD”)
“Other’s perceptions of TAD” and reactions to them; includes student and employer perceptions
“My classroom is mine,” a discourse that weaves multiple sections of Mark’s narrative:
His comments on personal as compared to professional trust
His “thick skin” discourse
TAD faculty characterizing each others’ classrooms as “over there”
“Mark’s question,” which begins as a question from Mark to the Olin faculty regarding team teaching, and also encompasses their replies to his question and each other’s answers
The “team teaching memories” discourse, which splinters into discourses based around different classes and faculty groupings (and intersects with “introductions” discourses)
“Mark and Gary” as a pairing for 130 as well as dialogue on other TAD courses
“Alan and Matt” as a pairing for 130
“The Collaboratory” at Olin, which expands beyond the dynamics of its faculty group into its own discourse on outreach and external visitors
“Rob the co-instructor” in a wide range of team teaching settings and pairings (includes “Jon and Rob,” largely as told from Rob’s perspective)
“Jon and Rob,” both within and outside “Stuff of History”
“How Stuff of History came to be” as an origin and an iterative evolution story
“The definition of design” -- different definitions of what that term can mean, and impressions of definition alignment
“Resolving language conflicts” -- tales of disagreement about language and what to do when it happens
“How design came to Olin” -- a tale of language realization in hindsight (parallels the “we’ve always been” discourse of TAD’s identity, which informs the discourse of “TAD’s Name”)
“Reflective processes,” which includes Rob’s comments on the rarity of engaging in this process, as well as Jon’s descriptions of his own reflective process (ties into “making sense in chaos”)
Rob’s image of “curriculum as a baton race,” where one faculty member teaches students a topic, and the next faculty member teaches them something that builds on that
“Who students are” and how they’ve changed, and the subsequent curricular design responses
“Who students should be,” (a contrast to “who students are”), which weaves through multiple desired student outcomes; habits of mind at TAD and the architect story at Olin
The “student perspective” and their reactions and complains about courses; extends into the challenges and possibilities of the various roles they could take in curricular design
“Students behind the curtain” seeing the mess; is it a good thing for faculty to enable or not? (Heavily overlapping with part of the “student perspective”)

### Using realtime transcription as a transcription technique

Qualitative data is typically audio-recorded and transcribed after the fact by a listening researcher. Since I am deaf, I use realtime transcription for phone interviews where I cannot lipread the speaker. This has the additional benefit of providing transcript access with a 5-second rather than 2-week or longer turnaround. At $60-120 per hour, the cost was comparable to outsourcing transcription. The process can be thought of as similar to hiring an interpreter for foreign-language interviews; a realtime captioner is essentially a speech-to-text interpreter.

Realtime transcription is a skilled service, not an automated software tool. The setup consists of a trained provider, typically with a stenographic keyboard, listening to the discussion and transcribing it in realtime onto a display for clients to view. Far more accurate than modern-day speech recognition software, captioners can handle accents, technical terminology, homophones, laughter and other non-word noises, speaker changes, and other auditorily difficult situations — such as my data collection, which features academics speaking rapidly and using technological, pedagogical, and psychological terms. I have written elsewhere about the benefits of using realtime transcription for qualitative research more generally (FIE PAPER CITATION).

Using realtime transcription also meant that I could specify and negotiate transcription conventions with the captioner ahead of time. There is not a strict standard format for realtime captioning output, but it tends to be similar to subtitle tracks for movies. Captioners do not typically note things that would require additional description, such as tone of voice, but they will use punctuation and note relevant non-verbal sounds such as long pauses, sighs, and laughter, as well as environmental noises that impact the conversation, such as ringing phones in the background. I also gave the captioner a list of anticipated vocabulary (acronyms, proper names, specialized terms, etc.) in advance to aid with understanding and correct spelling.

I was able to get realtime transcription for nearly all the interviews, but not all of them. Captioners typically require scheduling at least a week in advance, if not two. There were several occasions where narrators requested interview scheduling less than two weeks in advance, and I was unable to find a captioner in time for a few of those. Consequently, a small handful of interviews were done over Skype, audio-recorded, and then transcribed by a professional service and reviewed by both myself and the narrator for accuracy. This usually worked well, aside from the side effect of completely exhausting me for the remainder of the day, since it is hard work to participate in a conversation you can’t understand. The only visible hiccup was during Jon’s 5th interview:

JON: Wait, can you ask that -- When I said I -- What did I say? I didn't have a lot of --
MEL: I think I heard you saying that, "I'm an engineer. I don't have a lot of respect for students."
JON: Oh, maybe I said that, I don't know. Did I say that earlier when I was commenting to Mark or did I just say that?
MEL: Oh, just now. Maybe I misheard you.
JON: I think you might have misheard me.
MEL: I think I misheard you. My gut -- That didn't sound like something Jon would say.
JON: No.
MEL: And actually now I have a transcript, or soon to be a transcript record, of the reason why I typically try to use real time transcription.

This particular interview with Jon still worked in terms of yielding insightful data, since my probes were enough to continue eliciting narratives from him. Due to a lifetime of practice bluffing my way through hearing environments, I was typically able to keep the conversation going even if I didn’t have a full idea of what was being said. This meant that I didn’t have a clear picture of the stories he was telling me until a few days later when I got the transcript back and was able to understand our full conversation for the first time. However, except for the hiccup quoted above, the interview does not read differently from other interviews with realtime captioning, in terms of my apparent level of conversational understanding in the transcript. This is an indicator that the data quality of the transcripts without realtime captioning are equivalent to the ones with realtime captioning.

### Interview setup and data collection process

Schedule considerations with captioners were integrated into the interview preparation and setup process. The process was as follows:

I arranged a session time with the narrator and the captioner. When arranging a time with the captioner, I also sent them anticpated vocabulary and transcript formatting conventions.
Prior to the interview, I prepared the prompt and put it into Google Docs, a collaborative web-based document editor. I sent the prompt document to the narrator and captioner ahead of time, and set permissions on the document so they could view and edit it.
At the appointed time, I started a 3-way audio call with the narrator and captioner over Skype. I re-sent the prompt link to the narrator and captioner as a reminder and opened the prompt document on my own computer.
I asked the narrator to begin reviewing and responding to the prompt. The captioner began to type the spoken conversation in the collaborative document, immediately below the prompt.
As the interview conversation proceeded, I read the transcript on the screen, editing and annotating as necessarily (for instance, to correct the spelling of a name). Some captioners needed breaks in the middle long interviews; in this case, we simply paused the call for a few minutes, then resumed when they were ready again.
After the interview concluded, I thanked the narrator and captioner and disconnected the call.
I wrote a brief self-reflection on the call at the end of the transcript, then saved the document and sent it to the narrator for review and open-licensing (detailed in a later section).

There were two exceptions to this process. I conducted the first round of interviews with TAD narrators in person, still using realtime captioning. The process was identical, except that we called the captioner in using a speakerphone equivalent. The second exception to this process was Lynn, who preferred to type her interviews rather than speak and be transcribed. Consequently, Lynn’s interviews were conducted over Skype text chat; she and I are fast typists who can produce text at close to conversational speed.

### Data licensing and transparency process

As previously mentioned, the sites, events, and individuals involved in the study are so uniquely identifiable that to anonymize them would destroy narrative particularity (Bruner, YYYY, PP). In conversation with the narrators, I broached the idea of aiming for a radically transparent research process; instead of anonymizing data and keeping it public, what if we did exactly the opposite and made it public with full names attached? This would result in an open narrative corpus that could be shared and used for other work. In this way, part of the project "source code" and intermediate artifacts became not only readable but editable and reusable, enabling legitimate peripheral participation (Lave & Wenger, 1991).

By taking a radically transparent approach to qualitative research, this project contributes to the dialogue on rethinking scholarly impact. It decenters what Lather calls the "transformative intellectual" and "[interrupts] the dominance relationships in the academy of academics as the central "servants" by which emancipation/truth/etc happens" (1991, p. 47). With the academic researcher decentered, non-traditional audiences are invited to join the questioning of research practices and their effect on access; to participate in this questioning is also to participate in research itself.

For this study, I suggested that narrators release their transcripts under a Creative Commons Attribution Share-Alike license (Creative Commons, 2013). This license enables sharing and reuse under the conditions that original work be fully attributed and any remixes be released under a simiarly open license. All narrators took this suggestion when open-licensing their data.

Several features were needed to make an open data corpus a reality while protecting participants. First, even though narrators might be willing in principle to have ther interview data go public, they rarely know in advance exactly what they will say during an interview. Therefore, narrators should not be required to commit to open-licensing in advance; decisions should be made by narrators after the interview is over and they have had a chance to review their transcript. Second, narrators -- not researchers -- needed to be the ones in control of the decision to open-license their data. Finally, narrators needed the opportunity to edit their data before releasing it publicly.

The legal solution was to immediately transfer copyright to the narrator after the interview, then have them edit and open-license the version they wanted to make public. Having the narrator hold the copyright protects them, since only the copyright holder is allowed to apply a license to a work. This ensured the transcript version released would be a version the narrator was comfortable with.

In practice, narrators tended not to edit their transcripts at all. After conducting the interview, they were typically able to say that there wasn’t anything in those conversations that they would mind going public. The major exception to this was one of Jon’s 5th interview, where he referenced Lynn in describing his own personality. At the end of that interview, Jon mentioned that he felt uncomfortable with that reference and wanted to edit it: “Can I take out the reference to Lynn, and just keep the stuff in about myself?” 

Consequently, Jon and I spent the first part of his 6th interview editing out that section of the transcript until he was comfortable with it. The conversation we had while editing -- what to leave in, what to take out, how to make the remaining fragments make sense again -- became part of the transcript for Jon’s 6th interview. I asked Jon if he wanted to leave the editing conversation in the public dataset, and he said yes. This means that the traces of our removal process are clearly visible in the data. 

The majority of the data corpus has passed fully through the open-licensing process, and is available at https://github.com/mchua/facultyaslearners/. I had to remind and encourage narrators, typically via email, to complete the process after their interviews, as they would often forget. However, I’ve also mailed paper forms and self-addressed-stamped-envelopes, met narrators at conferences, visited their campuses, etc. with licensing forms to make it as easy as possible for them to sign off on making their data public. As of this time, Jon, Lynn, Rob, and Mark have all completed open-licensing all their transcripts. Gary and Alan’s transcripts are still going through the open-licensing process.

The creation of the public transcript can be regarded as an analytical act that highlights a faculty narrator’s agency in determining how to present themselves. It obeys Michael Apple’s exhortation to shift the researcher’s role “from being [a] universalizing spokesperson to acting as cultural workers whose task is to take away the barriers that prevent people from speaking for themselves” (Lather, 1991, p. ix) In terms of our research question about understanding faculty as learners , the public transcript shows us the data our narrators would like us to use in order to understand them, providing a starting point for the rest of my analysis of them as learners.

### Data corpus

Interviews took place over the course of approximately one year. Each session was scheduled to last 90 minutes. The only exception to this was the first session for each narrator, which had an additional 30 minutes at the start in order to give time to explain the study, answer questions, and do IRB paperwork.

Lynn, Jon, and Rob, the narrators from Olin, all conducted 6 interview sessions. Alan, Mark, and Gary, the narrators from TAD, all conducted 3 interview sessions due to an unexpectedly hectic semester and the schedule constraints therein -- every member of their department was teaching a severe overload that term in order to execute the curriculum design I was interviewing them about.

This resulted in a total of 27 interviews. However, one transcript (Jon’s 4th interview) was lost to technical failure, so the final data corpus was 26 interviews. Another transcript (Rob’s 6th interview) experienced technical failure but was reconstructed collaboratively by Rob and Mel immediately afterwards, and is included in the corpus.

The data corpus represents around 42 hours of data collection (26 transcripts x 1.5 hours each + 0.5 hours additional for each of 6 first interviews). In practice, this number is not exact; some interviews featured short breaks, some interviews ran over at the narrator’s request when they wanted to finish telling an extended story, some interviews started late because narrators were running late from a previous meeting, etc.

## Analysis process

The intersubjectivity of this study blurs the distinction between “data collection” and “data analysis.” Any boundaries drawn between “collection” and “analysis” or between different “stages” or “types” of analysis are agential (or Bohrian) cuts that enact agential separability, researcher-constructed distinctions between an analyzed phenomena and its external observer. Instead of speaking of data collection and analysis as separate phases, I will discuss it as a series of data transformations. In this section, I will revisit the postmodern toolbox from the literature review, but in more concrete terms. I will then go through several significant data transformation stages that I performed in this project.

### Using the postmodern toolbox

Throughout the analysis process, I used the postmodern tools discussed in the prior section to sensitize myself towards which data to focus on and how to interrogate it.

When you spot a metanarrative, proliferate it. When you see something claimed as truth or a story told as if it were the only one to be told, start hunting for indications of different truths and stories that contradict it. This is not in an attempt to prove the first metanarrative “wrong” and the different story “right,” or even the larger encompassing of both stories as “right.” Rather, it is to trouble our assumptions that we can accept metanarratives at all, and to get us into the habit of questioning them to see what sorts of complexities they may be helping us ignore.

Example: Mark tells the story of TAD previously having been scattered due to high turnover, but that now they are coming together into a cohesive unit. However, this story isn’t as simple as it seems. Where can you find indications that TAD faculty had been unified before and/or scattered now? The objective in these searches and proliferations is not to find “the truth” or to create a larger, all-encompassing metanarrative that includes all these stories, but to hold multiple conflicting truths in tension at the same time.


When you spot a Bohrian cut, blur it. When you see a categorization or a binary (which is simply a categorization into two parts), trouble the boundaries between those categories. Find cases that smear across them. Proliferate them into so many categories that it becomes impossible to simply place them in opposition.

For example, Lynn, Mark, and Rob all debate the question of whether students can contribute usefully to curricular design. A binary way of thinking about it would be to say that either students (1) can contribute or (2) cannot contribute; these two perspectives can be placed against each other in debate. A categorizing way of thinking about it would be to say that some students, at some times, can contribute -- and then proceed in trying to delineate and define which students contribute in which ways under what circumstances. Both these approaches are grounded in creating separations -- Bohrian cuts -- in order to eliminate that tension and uncertainty. The postmodern approach, again, walks deeper into tension by asking where in the narratives we see professors describing students being simultaneously helpful and not-helpful towards curricular design; this calls into question whether “helpful” and “not-helpful” are the only ways to look at it.


When you see signs and signifiers, trace their slippages and malleability. In the context of text narrative transcripts, signs most often mean words and phrases, and signifiers most often mean the meanings and definitions of those words and phrases. Recall from Cavallaro that signs are given meaning by their repeated use in different contexts (YYYY, PP) and therefore from the differences and variants and contradictions that pop up in those contexts. When a term occurs repeatedly and there seems to be a tension in the definition, resist following the impulse to remove the tension by chopping the term into sub-variants and pinning fixed meanings to them. Instead, pursue the differences that cause that tension as a way to gain and portray a richer appreciation for the term and how it shifts and moves in time and space. Note that pronouns and names also serve as signs in this regard.

For example, all the narrators give different definitions of the word “design.” Some give different answers that are more or less aligned with each other, and some attempt to specify different sub-variants of design (a form of categorization, which can also be challenged by our technique of blurring Bohrian cuts). How can we find these differences, attempt to proliferate and extend them, and present them as a partial picture of the constantly shifting dance of negotiating meaning and communication?

Pronouns and names/character referents form a specific subset of signs whose signifiers are illuminating to trace in a particular way. Ttracing the meaning of the pronouns “I” and “me” within a single interview can make-visible varying and sometimes conflicting perceptions of self. Tracing the pronouns “we” and “us” as compared to “they” and “them” make-visible portrayals of the boundary between the self and the other. In this study, tracing character referents is a subset of tracing signs and signifiers; however, Doucet and Mauthner (2008) created an entire method based on tracing voice and characters through pronouns. When applied specifically to pronouns and character referents, this technique highlights the narrative representations of community performance and produces a “cast list” of the characters in those narratives.

Tracing signs and signifiers helps us understand faculty-as-learners in several different ways. First of all, it portrays them as narrators who use the semiotic conventions of their culture to put together language that simultaneously assembles them (Parker, 2004, p. 90) as they weave their personal meaning-making into the narrative accrual. Secondly, when we trace the signs and signifiers they use to refer to themselves, we watch how they portray themselves as interacting with other characters in a narrative that takes place within their community of practice. This allows us to see their stories as folk tales of cognitive apprenticeship and situated learning. Finally, it helps us understand their narrative construction of reality as intersubjective; since faculty are iterating between telling and reading each other’s narratives of shared experiences, they appear frequently as characters within each other’s stories and reshape those characterizations as they go. “Wait, that’s how she sees me? Wait, that’s how he sees her? Let me rethink the way I see myself and them again.”


When you see texts, take writerly attitudes towards them. Keeping in mind Derrida’s broad definition that “all is text” (Derrida, YYYY), this means that a postsmodern analyst seeks to take a writerly attitude towards the world at large. Nothing is sacred truth that must remain untouched. All analysis is destructive in some way; but you can leave traces of the violence you’ve done so that others can retrace it.

A writerly attitude towards texts is a natural outgrowth of Bohrian epistemology. In commenting on Barad's tome on the topic, Michael Weinstein remarks that “the revolutionary nature of Bohrian epistemology lies in the importance of perspective,” wherein “individuals cannot divorce themselves from being within the universe itself; it is impossible to be an external observer. Engaging the world is no longer by Cartesian observation, but by active engagement... The revolutionary nature of Bohrian epistemology lies in the importance of perspective." (2015, p. 31) This means that instead of presenting themselves as somehow distinct or objectively separated from the process and data, I as a researcher explicitly present myself as inextricably meshed within it, to the point of incuding myself as a character in the discourses alongside the faculty narrators.

The most obvious example of this attitude in the data corpus comes from Rob’s 6th interview transcript. Unlike the other transcripts, it is not the direct transcription of the audio recording; without my knowledge, the recording setup failed partway through the interview. (I normally bring multiple physical recorders to an in-person interview, but since I was conducting this interview via Skype, I only used a single piece of software to record the call.) I discovered the mistake at the end of the call. After a few minutes of panic, I sat down and immediately reconstructed the dialogue between “Remembered Rob” and “Remembered Mel” as best I could, with commentary from “Narrator Mel” filling in the gaps and pointing out places where I wasn’t sure. I sent the reconstructed transcript to Rob, who edited “Remembered Rob” and added commentary from “Narrator Rob” on the reconstruction process. 

The resulting document has certainly done violence to the spoken conversation that did not get properly recorded, since it is not a verbatim reconstruction and contains additional commentary. It is difficult to place and categorize; it is both a reconstructed summary and a “real” dialogue; it is “raw data,” but also analysis that included steps beyond the typical data collection process. Instead of discarding this data as wrong or broken because it “broke protocol,” I’ve included it as part of the intriguing and complex mess that happens in the research process.

### Analysis during interviews: grounded indigenous coding and other forms of narrator participation

The usage of realtime captioning during the interview created opportunities to blur the line between data collection and analysis. At any point during an interview, the use of realtime captioning makes the full transcript of the session up to that point available to both the narrator and myself for commenting and editing. This enables us to engage in grounded indigenous coding, a new contribution to qualitative methodology that leads to opportunities for rich narrative intertextuality. 

Grounded indigenous coding combines the precision of member-checking grounded in verbatim transcripts with the immediacy of analyzing data while generating it, which Holstein and Gubrium call indigenous coding (1995). It is one of the main methodological contributions of this project. I have written more about grounded indigenous coding in other locations (FIE paper on CART, 2014).

Although these speech acts take place naturally during conversation, I also deliberately choose to make them visible to faculty narrators whenever possible, subverting the usual researcher/subject power dynamic. By naming these speech acts as moments of co-analysis, indigenous coding transforms narrators from “subjects” to “fellow researchers.” 

Even if all the narrators for this study have done educational research as a non-trivial part of their faculty jobs, indigenous coding was an unique experience. Rob expressed it most concisely in his 6th interview (as best as we could remember in reconstruction):

REMEMBERED ROB: I never get to do this kind of analysis, because when I read things from other faculty, it's usually the finished final product. But here is the raw stuff, and it's fascinating to see the raw stuff when it's not done yet, and to be able to comment on that as it is unfolding, and hear a response to my comment or observation.

I will now walk through a concrete example of grounded indigenous coding before unpacking it with prior literature and examining its significance as a methodological contribution. The following example is  drawn from the start of Jon’s first interview, where he introduces himself:

JON: My name is Jon Stolk. I have been at Olin since 2001... I got to Olin a little bit before students arrived.

Approximately 40 minutes later, during the same narrative session, Jon reads the above line in his own transcript and begins a self-analysis with the following comment:

JON: It is important for me to communicate to people that I arrived before students arrived at Olin. I notice that I always do this. I always have to get that into my introduction.

In some ways, this is similar to prompt creation; it is also linking-into-discourse, but in this case a discourse is linking to itself. Jon is clarifying and reflecting upon what he’s saying while he’s saying it, which Holstein and Gubrium classify as a form of data analysis called indigenous coding. Phrases like “you mean...” or “to sum it up...” or “one way of thinking about it…” are typical signals of indigenous coding (Holstein & Gubrium, 1995, p. 56). Narrators see new patterns while reflecting on the stories they have just told, occasionally eliciting further thoughts.

Jon’s self-analysis is also grounded in his verbatim transcript, in contrast to the memory-based nature of most indigenous coding. Having the transcript as a concrete boundary object allowed precise, detailed analysis of phrasings and re-phrasings, specific vocabulary, and things the narrator may not realize they’ve said. I was able to scroll back and tell a narrator exactly what they or I had said earlier in the conversation, and often used this ability as a probe technique, as shown in the example below.

MEL: Ok. I'd like to circle back to something you said at the start of today. (Quoting Lynn verbatim from the transcript of the conversation we were having) "Anyway, I've finished reading the identity chunk.  Happy to discuss further, or I can go on to the next chunk."

Lynn and I had previously chosen to “go on to the next chunk” (of the prompt). Now, having been reminded of her earlier thought of circling back, Lynn chose to go back to the question of identity, and we spent the remaining 30% of her interview on that topic.

It also allowed us to return to missed comments that were relevant to later conversation:

LYNN: [I just realized] that Jon has no idea of when I am being sarcastic.
MEL: That was the part I wondered about earlier in this conversation when I said "I think Jon missed your humor there."
LYNN: missed Mel's comment about Jon missing humor.

In the above case, the following discussion led to Lynn realizing new things about the relationship and communications dynamics between herself and Jon. Questions of what else she might have missed and where else she might be missing it came up in the discussion. Having the conversation transcript during the conversation itself allowed us both to scroll up and down to see if we had missed other things.

I had hoped that realtime captioning would also allow the interview to function as a preliminary member check of transcript accuracy, a technique used in some qualitative studies to confirm transcript accuracy and involve subjects more deeply in the co-creation of knowledge. Transcription turnaround times typically require member checks be scheduled as separate sessions, trading immediacy and scheduling convenience for a grounded analysis. I had hoped that removing transcription turnaround times as a restriction would allow narrators to approve their transcript immediately at the conclusion of an interview. 

In practice, this only turned out to be the case for a few interviews. Overwhelmingly, the new restriction to appear was simply that narrators wanted more time to think about how they felt about what they’d said. However, having realtime transcription meant they could take their transcripts for review immediately, while it was still fresh in their heads. Member check turnaround time was still shortened, but not as completely as I would have hoped.

By reducing transcript turnaround time to 5-10 seconds (the average remote captioning delay), we remove the tradeoff between immediacy and precision. By making transcription an important study design element, we uncover it as a component of research methodology whose effects on data often go ignored and unthought-of (Lapadat &Lindsay, 1999). By not temporally and spatially separating “data collection” from “data analysis,” we also reveal the arbitrary nature of the slice researchers often make to separate the two.

### Analysis before interviews: prompt creation / linking into discourses

Another stage of data transformation occurred before each interview, where I linked discourses together into prompts. Linking discourses consists of taking and remixing existing texts. In the case of most interview prompts, I drew entirely from transcripts in the existing data corpus, adding only header information. The 6th interview prompt also drew from a portion of my intermediate results. However, even the simple two-line prompt for the first interview is a discourse; even if there was no data in the corpus to draw from, each line of the prompt was inspired by numerous prior open-ended narrative interviews. These are also texts, albeit common and generic enough as to remain uncited here.

The next few paragraphs walk through a partial example of creating a prompt. Lynn was asked in her first session how she would tell the story of the design thinking curriculum at Olin. Part of her response was as follows:

LYNN: [If I were to write a book about the curriculum revision,] Chapter 1 [would be called] "We need a design experience in the fourth semester."

This fragment of text became part of the prompt for Rob’s third interview. In response to this section of his prompt, Rob responded:

ROB: Chapter 1, she is saying there is some need for design experience. That is fascinating for me to hear. I wasn’t aware of any of the details of that discussion.

Both fragments above were included, next to each other, as part of the prompt for Jon’s third interview. In other words, Jon’s prompt contained both Lynn’s transcript and Rob’s reply to Lynn’s transcript. Jon responded to both these sections during the course of his interview::

JON: So what Lynn said is exactly what I remember. I think I described this in my last interview or maybe the first one... It’s interesting to see Rob’s comments too that he wasn’t aware of the detail...

Jon’s interview transcript was then taken and remixed into future prompts and discourses, and so forth. 

As the above example begins to show, this process blurs the distinction between data collection and analysis. It also blurs the distinction between researcher and subject, since using these prompts for the interviews leads to faculty narrators co-analyzing their data. The resulting emerging narrative accrual consists of interlinked narratives that overlap, intertwine, and make-meaning-of each other.

Linking text fragments into discourses also challenges notions of “raw data” and what is “real” as opposed to “fictional.” In creating some discourse fragments for prompts, I place fragments of conversation from individual transcripts in direct conversation with each other, a technique also employed by Talmudic scholars as a way of making and remaking sense of an ancient text in the authors’ here and now (Talmud book, PP).  These conversations occurred with different people in different times and places, so they only “happened” on the page. However, to say they never happened in “reality” is also to say the discourse fragments -- my interview prompts -- do not exist in reality.

Similarly, in keeping with Barad’s idea of the enmeshed nature of the observer and the observed, I expand and edit my own voice in discourse fragments. In this way, I explicitly present myself as another character in the dialogues, instead of speaking only as an omniscient narrator. One vivid example of this taken to an extreme is within Rob’s 6th transcript, which was lost and then reconstructed immediately afterwards by myself and Rob. In this transcript, both Rob and I (Mel) portray ourselves as fallible narrators reconstructing dialogue from our past selves.

NARRATOR MEL: “Narrator Mel” and “Narrator Rob” are us reconstructing the transcript in the present, which (for us) is the morning of January 24 through the evening of January 25, 2015. “Remembered Mel” and “Remembered Rob” are what we think we remember ourselves saying, verbatim or close to verbatim, during the original interview in the evening of January 23, 2015.

Note that we attempt to leave traces of the violence we are doing to the text in “reconstructing the transcript in the present,” and questioning the “truth” of a reconstruction that uses “what we think we remember ourselves saying.” In distinguishing our “narrator” selves as different voices in the text, we make it clear that the reader should be skeptical of treating this as a verbatim account.

REMEMBERED ROB: It seems that I have a bit of a reputation for this approach and maybe it is a part of my identity, as perceived by my friends and colleagues...

NARRATOR MEL: Rob noted that all three Olin faculty seemed to be having these moments of reflection and identification and sensemaking of themselves and their relationships with one another… I wish I had the recording because Rob put it so beautifully, broadening the scope of that one comment from Lynn, and I'm just not capturing that here.

NARRATOR ROB: It is true that my words were a thing of beauty, a gloriously poetic breath of philosophy that held the promise of widespread societal enlightenment.  Sadly, I too do not recall the exact words used but I am pretty certain that we captured the essence of it here.

I used text similar to the above in creating some of the prompts. In fact, Lynn received a prompt containing nearly 2 pages of “Narrator Mel” and “Narrator Rob;” after her interview, Jon received a prompt containing portions of the same 2 pages interspersed with Lynn’s commentary on them. Sometimes interviews felt like stepping into a rabbit hole in Alice in Wonderland because of the many levels of complex cross-references in our conversations.

Prompt creation is the most explicit instance of linking into discourses, but I continued linking discourses through the entirety of the project. In many ways, all data analysis and transformation can be seen as linking into discourse; this document itself is a discourse remixed from many texts (as evidenced by the citations section). The process never ends; we simply stop writing it.

### Analysis between and after interviews: creating rhizomatic fragments

Rhizomatic fragments can be thought of as extremely short and “violent” versions of the linked discourses discussed earlier. They exhibit heavy and obvious modifications of the “raw” data used to produce them. Like discourses, rhizomatic fragments highlight the sheer magnitude of the many ways we can understand faculty-as-learners. 

The term “rhizomatic fragment” was inspired by the Capitalism and Schizophrenia project’s description and example of poststructuralist theory and research as a nonhierarchical rhizome that can be entered, navigated, represented, and exited from many points, in many ways, and with many interpretations (Deleuze & Guattari, 1987; Deleuze & Guattari, 2009).

Rhizomatic fragments are intentionally short so that they can be inserted into the flow of a document, rapidly read, and more easily compared with each other. They are bite-sized artifacts for challenging our initial readings of a text. In order to play with different representations of the same material, rhizomatic fragments can take many formats; they can be graphical, translated into another language, structured as sonnets, and so forth.

We can create an example rhizomatic fragment with the intertextual transcript excerpts used earlier to illustrate how I built prompts:

Lynn said that Chapter 1 [of a book about Olin's curriculum revision would be called] "We need a design experience in the fourth semester." That is exactly what Jon remembers. Rob wasn't aware of any of the details, so Jon thinks it’s interesting to see Rob's comments.

As noted above, we can create more than one rhizomatic fragment with the same tiny set of quotes. Note the difference in formats between the previous fragment and the following one.

LYNN: We need a design experience in the fourth semester.
ROB: She is saying there is some need for design experience.
JON: So what Lynn said is exactly what I remember. [Rob]...
ROB AND JON: ...wasn't aware of the details.
ROB: That is fascinating for me to hear.
JON: It's interesting to see Rob's comments too.

Rhizomatic fragments engage us in deconstruction, “[encouraging] a multiplicity of readings by demonstrating how we cannot exhaust the meaning of the text, how a text can participate in multiple meanings without being reduced to any one, and how our different positionalities affect our reading of it" (Lather, 1991, p. 145). The rhizomatic fragment examples above drew from a tiny set of just three speaker utterances. Access to the full dataset of 26 narrative session transcripts, each with hundreds of speaker utterances, opens up the possibility of many more. Each of this study’s narrative session prompts can be considered a collection of rhizomatic fragments;they are remixes of excerpts from the previous sessions of some subset of the interviews.

Authoring these fragments, and explicitly employing radically transparent practices so others can do the same, helps address the research question of understanding faculty-as-learners by proliferating a host of ways to see them as learners. A poststructural narrative accrual composed of rhizomatic fragments does not need to converge upon a single version of “truth.” On the contrary, it “[creates] a multiply voiced text that accumulate[s] meaning as the text proceed[s] in a way that goes beyond the pages of the book” (Lather, 2008, p. 2). Authoring rhizomatic fragments can be seen, via an agential cut, as a form of analysis.Far from being a “failure of interpretive responsibility” to “analyze… what [the narrators’ words] really meant” (p. 2), the rhizomatic fragments produced by linking discourses highlight the pluralities of ways we can constantly make and re-make our understanding.
